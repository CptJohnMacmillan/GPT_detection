# GPT Detection Project

## Overview

This project involves the analysis of 1500 student essays from the Persuade corpus, which were further subsetted to include only texts with word counts between 250-400 words. This was done to maintain consistency with the ArguGPT dataset, where models were prompted to write approximately 300 words. The goal of this project is to build a model that can differentiate between human-written essays and those generated by GPT models.

## Dataset

The dataset includes:
- **Persuade corpus**: A collection of 1500 mediocre to well-written student essays (scores 3-5), with each essay having a word count between 250-400 words.
- **ArguGPT dataset**: A collection of 1500 GPT-generated essays, each with a word count between 250-400 words.

Both datasets were combined for training and evaluation purposes.

## Methods

The following steps were taken in the notebook to build and evaluate the model:

1. **Data Preprocessing**: The text data was cleaned and vectorized.
2. **Model Training**: A RandomForestClassifier with GridSearchCV was used for training.
3. **Feature Importances**: The most important features for the model were identified.
4. **Model Evaluation**: The performance of the model was evaluated using various metrics.

### Models Used

#### General Model

- **RandomForestClassifier with GridSearchCV**

#### Text Features Model

- **RandomForestClassifier focusing on text features**

#### Stylistic Features Model

- **RandomForestClassifier focusing on stylistic features**

#### Punctuation Model

- **RandomForestClassifier focusing on punctuation features**

## Results

### Model Performance Metrics

| Model                    | Precision (Class 0) | Precision (Class 1) | Recall (Class 0) | Recall (Class 1) | F1-Score (Class 0)  | F1-Score (Class 1)  | Accuracy |
|--------------------------|---------------------|---------------------|------------------|------------------|---------------------|---------------------|----------|
| General Model            | 0.95                | 0.99                | 0.99             | 0.95             | 0.97                | 0.97                | 0.97     |
| Text Features Model      | 0.92                | 0.94                | 0.93             | 0.93             | 0.93                | 0.94                | 0.93     |
| Stylistic Features Model | 0.90                | 0.95                | 0.95             | 0.90             | 0.92                | 0.93                | 0.93     |
| Punctuation Model        | 0.85                | 0.74                | 0.65             | 0.90             | 0.73                | 0.81                | 0.78     |

### Feature Importances

From the Random Forest model focusing on punctuation features, the most important features identified were:

1. **Punctuation Errors**: Importance - 0.837
2. **Punctuation Count**: Importance - 0.163

### Confusion Matrix for General Model

The confusion matrix for the general model is shown below:

|                  | precision | recall | f1-score | support |
|------------------|-----------|--------|----------|---------|
| Class 0          | 0.95      | 0.99   | 0.97     | 283     |
| Class 1          | 0.99      | 0.95   | 0.97     | 317     |
| **Accuracy**     |           |        | 0.97     | 600     |
| **Macro avg**    | 0.97      | 0.97   | 0.97     | 600     |
| **Weighted avg** | 0.97      | 0.97   | 0.97     | 600     |

## Conclusion

The models were able to achieve a good level of accuracy in distinguishing between human-written and GPT-generated essays. The feature importances and performance metrics indicate that punctuation-related features were significant in making these distinctions.

## Files

- `GPT_Detection_Project_Final.ipynb`: The Jupyter notebook containing the full analysis and results.

## References

- [Persuade corpus](https://www.kaggle.com/datasets/nbroad/persaude-corpus-2)
