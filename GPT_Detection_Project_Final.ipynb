{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc1a22a",
   "metadata": {},
   "source": [
    "## GPT detection project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4aeec",
   "metadata": {},
   "source": [
    "Information about the project and types of features aggregated to make predictions.\n",
    "\n",
    "This project aims to detect texts generated by chatGPT(3.5) as opposed to texts written by humans. To do this we will train a random forest model on 3 groups of features, and will as well compare performance on the full set of features. We hypothesize that groups of features described below will allow for reasonable division of chatGPT and human written essays.\n",
    "\n",
    "1. general text features - these focus on lengths of texts, paragraphs and sentences. These features inform us about the basic complexity of text (simple vs. complex sentences) as well as the propensity to write shorter or longer texts on average.\n",
    "\n",
    "2. stylistic features - quantifiable features that are mostly focused on the uniquness of text. Therefore, we measure numbers of unique words, repetitions, words that belong to certain categories (such as discourse markers, stopwords or pronouns) as well as sentiment and readability.\n",
    "\n",
    "3. punctuation - two features - punctuation count and punctuation errors - are supposed to measure the propensity to use punctuation and measure whether it's used correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc4fd5",
   "metadata": {},
   "source": [
    "The project is structured as follows:\n",
    "\n",
    "1. low level functions to be used later on \"text\", \"paragraphs\" and \"sentences\" columns.\n",
    "\n",
    "2. Low level functions are aggregated for convenience in bigger functions that enable the user to get all features for the \"text\", \"paragraphs\" or \"sentences\" at once. These tools are not very computationally intensive, which means that you can make those calls on big amounts of data as well.\n",
    "\n",
    "3. Resulting dataframe is split according to aforementioned features in order to run separate models and test features utility. For this purpose, the notebook is divided into specific sections.\n",
    "\n",
    "4. Each section displays important metrics for features of the model.\n",
    "\n",
    "5. Final tables summarize performance on separate groups of features and the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import language_tool_python\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe883b",
   "metadata": {},
   "source": [
    "## Project functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af93c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining subfunctions to be used with other functions\n",
    "\n",
    "# Tokenizing words (nltk tokenizer) and removing tokens that contain punctuation, like \".\"\n",
    "def tokenize_and_remove(text):\n",
    "    \n",
    "    tokenized_text = word_tokenize(text)\n",
    "    words = list(filter(lambda token: token not in string.punctuation, tokenized_text))\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Counting words - a single paragraph should be passed or iteration through paragraphs\n",
    "def count_word_paragraph(para):\n",
    "    word_count = []\n",
    "    for x in para:\n",
    "        words = word_tokenize(x)\n",
    "        words = list(filter(lambda token: token not in string.punctuation, words))\n",
    "        words_len = len(words)\n",
    "        word_count.append(words_len)\n",
    "        \n",
    "    return word_count\n",
    "\n",
    "# Counting sentences in a paragraph - similar to the above function for words\n",
    "def count_sent_paragraph(para):\n",
    "    sent_count = []\n",
    "    for x in para:\n",
    "        sents = sent_tokenize(x)\n",
    "        sents_len = len(sents)\n",
    "        sent_count.append(sents_len)\n",
    "        \n",
    "    return sent_count\n",
    "\n",
    "# Counting words in a sentence. A sentence should be passed or iteration through sentences\n",
    "def words_per_sentence(sent):\n",
    "    word_count = []\n",
    "    for x in sent:\n",
    "        words = word_tokenize(x)\n",
    "        word_list = list(filter(lambda token: token not in string.punctuation, words))\n",
    "        words_len = len(word_list)\n",
    "        word_count.append(words_len)\n",
    "        \n",
    "    return word_count\n",
    "\n",
    "# Counting unique words per sentence - using set on the previous function.\n",
    "def unique_words_per_sentence(sent):\n",
    "    word_count = []\n",
    "    for x in sent:\n",
    "        words = word_tokenize(x)\n",
    "        word_list = list(filter(lambda token: token not in string.punctuation, words))\n",
    "        word_set = set(word_list)\n",
    "        words_len = len(word_set)\n",
    "        word_count.append(words_len)\n",
    "        \n",
    "    return word_count\n",
    "\n",
    "# Returning the number of unique pos tags (using pos_tag from NLTK)\n",
    "def unique_pos_tags_per_sentence(para):\n",
    "    pos_count = []\n",
    "    for sent in para:\n",
    "        tags = len(set(pos_tag(word_tokenize(sent))))\n",
    "        pos_count.append(tags)\n",
    "        \n",
    "    return pos_count\n",
    "\n",
    "# Returning unique words from the whole text\n",
    "def unique_words_per_text(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    word_list = list(filter(lambda token: token not in string.punctuation, words))\n",
    "    word_set = set(word_list)\n",
    "        \n",
    "    return word_set\n",
    "\n",
    "# Returning the relative number of unique words from the whole text\n",
    "def uniq_words_relative(text):\n",
    "    words = word_tokenize(text)\n",
    "    word_list = list(filter(lambda token: token not in string.punctuation, words))\n",
    "    unique_words = set(word_list)\n",
    "    total_words = len(word_list)\n",
    "    uniq_words_rel = len(unique_words) / total_words\n",
    "    \n",
    "    return uniq_words_rel\n",
    "\n",
    "# Cleaning text by replacing next line and next paragraph markers\n",
    "def clean_text(text):\n",
    "    \n",
    "    cleaned_text = text.replace('\\n\\n', ' ')\n",
    "    cleaned_text = cleaned_text.replace('\\n', ' ')\n",
    "    return cleaned_text\n",
    "\n",
    "# Counting stopwords using stopwords from NLTK\n",
    "def get_stopwords_count(text):\n",
    "    \n",
    "    stops = stopwords.words('english')\n",
    "    \n",
    "    count = 0\n",
    "    words = word_tokenize(text)\n",
    "        \n",
    "    for word in words:\n",
    "        if word in stops:\n",
    "            count += 1\n",
    "        \n",
    "    return count\n",
    "\n",
    "# Returning a repetition score. Repetitions are penalized as squared counts. We divide the final score \n",
    "# by how many words appeared in the text.\n",
    "def repetition(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in string.punctuation and word not in stop_words]\n",
    "    token_counts = Counter(tokens)\n",
    "    token_list = list(token_counts.items())\n",
    "    new_list = list(x[1] for x in token_list)\n",
    "    repet_vec = (np.sum(((np.array(new_list)) ** 2))) / len(new_list)\n",
    "    \n",
    "    return repet_vec\n",
    "\n",
    "# get number of punctuation errors. Define a tool (ie. lang = language_tool_python.LanguageTool('en-US'))\n",
    "# and pass lang.check(text). In place of text put your text. Remember to close with lange.close()\n",
    "\n",
    "def punctuation_error(text):\n",
    "    \n",
    "    count = 0\n",
    "    for entry in lang.check(text):\n",
    "        if entry.category == 'PUNCTUATION':\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Check elements of l2 against l1 and return the length of the list.\n",
    "counter = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronoun_counts(text):\n",
    "    \n",
    "    # Declaring pronouns \n",
    "    pronouns = ['I', 'You', 'He', 'She', 'It', 'We', 'They', 'Me', 'You', 'Him', 'Her', 'It', 'Us', \n",
    "            'Them', 'My', 'Your', 'His', 'Her', 'Its', 'Our', 'Their', 'Mine', 'Yours', 'His', \n",
    "            'Hers', 'Its', 'Ours', 'Theirs', 'Myself', 'Yourself', 'Himself', 'Herself', 'Itself', \n",
    "            'Ourselves', 'Yourselves', 'Themselves', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'you', \n",
    "            'him', 'her', 'it', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their', 'mine', \n",
    "            'yours', 'his', 'hers', 'its', 'ours', 'theirs', 'myself', 'yourself', 'himself', 'herself', \n",
    "            'itself', 'ourselves', 'yourselves', 'themselves']\n",
    "    \n",
    "    multi_word_pronouns = ['Each other', 'One another', 'each other', 'one another']\n",
    "    \n",
    "        \n",
    "    token_words = word_tokenize(text)\n",
    "    words = list(filter(lambda token: token not in string.punctuation, token_words))\n",
    "    pronoun_count = 0\n",
    "        \n",
    "    # Iterate over each pronoun and increment pronoun count\n",
    "    for pronoun in multi_word_pronouns:\n",
    "        count_pro = text.count(pronoun)\n",
    "        pronoun_count += count_pro\n",
    "        \n",
    "    for word in words:\n",
    "        if word in pronouns:\n",
    "            pronoun_count += 1\n",
    "        \n",
    "        # Get the relative pronoun count \n",
    "        relative_pronoun_count = pronoun_count / len(words)\n",
    "        \n",
    "    return pronoun_count, relative_pronoun_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77856668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discourse_count(text):\n",
    "    \n",
    "    # Declaring lists of discourse markers\n",
    "    one_disc_mark = ['And', 'Also', 'Moreover', 'Furthermore', 'Plus', 'Besides', 'Additionally', 'But', \n",
    "                 'However', 'Nevertheless', 'Nonetheless', 'Whereas', 'While', 'Although', 'Though', \n",
    "                 'Yet', 'Because', 'Since', 'As', 'Therefore', 'Thus', 'Hence', 'Consequently', 'So', \n",
    "                 'Therefore', 'Thus', 'Hence', 'Consequently', 'Accordingly', 'So', 'Therefore', 'Like', \n",
    "                 'Namely', 'Namely', 'Specifically', 'First', 'Second', 'Third', 'Next', 'Then', \n",
    "                 'Finally', 'Lastly', 'Meanwhile', 'Meanwhile', 'Before', 'After', 'While', 'During', \n",
    "                 'Meanwhile', 'Since', 'Until', 'Afterwards', 'Then', 'Next', 'Later', 'Soon', 'Finally', \n",
    "                 'Ultimately', 'Indeed', 'Certainly', 'Absolutely', 'Definitely', 'Surely', 'Undoubtedly', \n",
    "                 'Conversely', 'Notwithstanding', 'However', 'Nevertheless', 'Definitely', 'Absolutely', \n",
    "                 'Certainly', 'Indeed', 'Clearly', 'Undoubtedly', 'and', 'also', 'moreover', \n",
    "                 'furthermore', 'plus', 'besides', 'additionally', 'but', 'however', 'nevertheless', \n",
    "                 'nonetheless', 'whereas', 'while', 'although', 'though', 'yet', 'because', 'since', \n",
    "                 'as', 'therefore', 'thus', 'hence', 'consequently', 'so', 'therefore', 'thus', 'hence', \n",
    "                 'consequently', 'accordingly', 'so', 'therefore', 'like', 'namely', 'namely', \n",
    "                 'specifically', 'first', 'second', 'third', 'next', 'then', 'finally', 'lastly', \n",
    "                 'meanwhile', 'meanwhile', 'before', 'after', 'while', 'during', 'meanwhile', 'since', \n",
    "                 'until', 'afterwards', 'then', 'next', 'later', 'soon', 'finally', 'ultimately', \n",
    "                 'indeed', 'certainly', 'absolutely', 'definitely', 'surely', 'undoubtedly', 'conversely', \n",
    "                 'notwithstanding', 'however', 'nevertheless', 'definitely', 'absolutely', 'certainly', \n",
    "                 'indeed', 'clearly', 'undoubtedly']\n",
    "    \n",
    "    multi_disc_mark = ['In addition', 'On the other hand', 'In contrast', 'Due to', 'Owing to', 'As a result', \n",
    "                   'For example', 'For instance', 'Such as', 'In other words', 'That is to say', \n",
    "                   'To clarify', 'Put differently', 'After that', 'At last', 'In conclusion', \n",
    "                   'To sum up', 'In summary', 'Of course', 'On the contrary', 'in addition', \n",
    "                   'on the other hand', 'in contrast', 'due to', 'owing to', 'as a result', \n",
    "                   'for example', 'for instance', 'such as', 'in other words', 'that is to say', \n",
    "                   'to clarify', 'put differently', 'after that', 'at last', 'in conclusion', \n",
    "                   'to sum up', 'in summary', 'of course', 'on the contrary'] \n",
    "    \n",
    "    token_words = word_tokenize(text)\n",
    "    words = list(filter(lambda token: token not in string.punctuation, token_words))\n",
    "    disc_count = 0\n",
    "        \n",
    "    # Iterate over discourse markers to get their count\n",
    "    for phrase in multi_disc_mark:\n",
    "        count_disc = text.count(phrase)\n",
    "        disc_count += count_disc\n",
    "        \n",
    "    for word in words:\n",
    "        if word in one_disc_mark:\n",
    "            disc_count += 1\n",
    "        \n",
    "    return disc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    \n",
    "    # get sentiment values and extract polarity and subjectivity\n",
    "    sent_values = TextBlob(text)\n",
    "    polar = sent_values.sentiment.polarity\n",
    "    subject = sent_values.sentiment.subjectivity\n",
    "    \n",
    "    return polar, subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readability(text):\n",
    "    \n",
    "    # get reading ease and kincaid grade metrics\n",
    "    ease = flesch_reading_ease(text)\n",
    "    grade = flesch_kincaid_grade(text)\n",
    "        \n",
    "    return ease, grade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6dde7",
   "metadata": {},
   "source": [
    "## Aggregated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_para_and_sents(text, orig_df):\n",
    "    \n",
    "    # Splitting into paragraphs on the \"\\n\\n\" sequences, which usually denote a new paragraph. \n",
    "    # This might be a bad implementation if there is no \"n\\n\\\" sequence for a new paragraph.\n",
    "    paragraphs = [x.split('\\n\\n') for x in text]\n",
    "    # Splitting the text into sentences (using sent_tokenize from nltk)\n",
    "    sentences = [sent_tokenize(x) for x in text]\n",
    "    \n",
    "    # Creating a dataframe to merge with the original dataframe\n",
    "    df = pd.DataFrame({'paragraphs': paragraphs,\n",
    "                       'sentences': sentences})\n",
    "    \n",
    "    # Merging both dataframes\n",
    "    returned_df = pd.concat([orig_df, df], axis=1)\n",
    "    \n",
    "    return returned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4fd625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will be applying our low level functions to get features based on paragraphs\n",
    "# This function requires a columns with lists of paragraphs and the original dataframe to be passed\n",
    "def get_paragraph_features(paragraphs, orig_df):\n",
    "    \n",
    "    # initalizing empty lists for means and stds, doing list comprehension to get paragraph count\n",
    "    paragraph_count = [len(para) for para in paragraphs]\n",
    "    words_per_para_mean = []\n",
    "    words_per_para_std = []\n",
    "    sentences_per_para_mean = []\n",
    "    sentences_per_para_std = []\n",
    "    \n",
    "    # Iterating through paragraphs, getting means and stds, appending them to the lists\n",
    "    for para in paragraphs:\n",
    "        \n",
    "        words = count_word_paragraph(para)\n",
    "        sents = count_sent_paragraph(para)\n",
    "        \n",
    "        words_mean = np.mean(words)\n",
    "        words_std = np.std(words)\n",
    "        \n",
    "        sents_mean = np.mean(sents)\n",
    "        sents_std = np.std(sents)\n",
    "        \n",
    "        words_per_para_mean.append(words_mean)\n",
    "        words_per_para_std.append(words_std)\n",
    "        sentences_per_para_mean.append(sents_mean)\n",
    "        sentences_per_para_std.append(sents_std)\n",
    "    \n",
    "    # creating a dataframe from our features to merge it with our original dataframe\n",
    "    para_df = pd.DataFrame({'paragraph_count': paragraph_count,\n",
    "                            'words_per_paragraph_mean': words_per_para_mean, \n",
    "                            'words_per_paragraph_std': words_per_para_std, \n",
    "                            'sentences_per_paragraph_mean': sentences_per_para_mean, \n",
    "                            'sentences_per_paragraph_std': sentences_per_para_std})\n",
    "    \n",
    "    # merging the dataframe with the orignal one\n",
    "    returned_df = pd.concat([orig_df, para_df], axis=1)\n",
    "    \n",
    "    return returned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting sentence features - we should pass a column with lists of sentences and our original dataframe\n",
    "def get_sentence_features(sentences, orig_df):\n",
    "    \n",
    "    # Initalizing empty lists and getting sentence count \n",
    "    sentence_count = [len(sent) for sent in sentences]\n",
    "    words_per_sent_mean = []\n",
    "    words_per_sent_std = []\n",
    "    uniq_words_per_sent_mean = []\n",
    "    uniq_words_per_sent_std = []\n",
    "    pos_tags_per_sentence_mean = []\n",
    "    \n",
    "    # Iterating through paragraphs to get the features \n",
    "    for sent in sentences:\n",
    "        \n",
    "        words = words_per_sentence(sent)\n",
    "        uniq_words = unique_words_per_sentence(sent)\n",
    "        pos_tags = unique_pos_tags_per_sentence(sent)\n",
    "        \n",
    "        words_mean = np.mean(words)\n",
    "        words_std = np.std(words)\n",
    "        \n",
    "        uniq_words_mean = np.mean(uniq_words)\n",
    "        uniq_words_std = np.std(uniq_words)\n",
    "        \n",
    "        pos_tags_mean = np.mean(pos_tags)\n",
    "        \n",
    "        words_per_sent_mean.append(words_mean)\n",
    "        words_per_sent_std.append(words_std)\n",
    "        uniq_words_per_sent_mean.append(uniq_words_mean)\n",
    "        uniq_words_per_sent_std.append(uniq_words_std)\n",
    "        pos_tags_per_sentence_mean.append(pos_tags_mean)\n",
    "    \n",
    "    # creating a dataframe from our features to merge it with our original dataframe\n",
    "    sent_df = pd.DataFrame({'sentence_count': sentence_count,\n",
    "                            'words_per_sent_mean': words_per_sent_mean, \n",
    "                            'words_per_sent_std': words_per_sent_std, \n",
    "                            'uniq_words_per_sent_mean': uniq_words_per_sent_mean, \n",
    "                            'uniq_words_per_sent_std': uniq_words_per_sent_std,\n",
    "                            'pos_tags_per_sentence_mean': pos_tags_per_sentence_mean})\n",
    "    \n",
    "    # merging the dataframe with the orignal one\n",
    "    returned_df = pd.concat([orig_df, sent_df], axis=1)\n",
    "    \n",
    "    return returned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf92bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE TO RUN LANGUAGE TOOL LOCALLY - OTHERWISE LANGUAGE TOOL DOESN'T WORK\n",
    "# lang = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90738cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close it when you finish\n",
    "# lang.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting features based on the whole text - we pass a columns with the whole texts and the original dataframe\n",
    "def get_text_features(text, orig_df):\n",
    "    \n",
    "    # counting words in the text using our custom function that removes punctuation tokens\n",
    "    word_count = [len(tokenize_and_remove(x)) for x in text]\n",
    "    # counting unique words using set\n",
    "    uniq_word_count = [len(set(tokenize_and_remove(x))) for x in text]\n",
    "    # counting relative number of unique words\n",
    "    uniq_word_count_rel = [len(set(tokenize_and_remove(x))) / len(tokenize_and_remove(x)) for x in text]\n",
    "    # counting punctuation using our custom function \"counter\"\n",
    "    punctuation_count = [counter(x, string.punctuation) for x in text]\n",
    "    # getting the number of stopwords in a text\n",
    "    stopwords_count = [get_stopwords_count(x) for x in text]\n",
    "    # getting the number of discourse markers in a text\n",
    "    discourse_count = [get_discourse_count(x) for x in text]\n",
    "    # getting the number of pronouns and relative \n",
    "    pronouns = text.apply(get_pronoun_counts)\n",
    "    total_pronoun_count, relative_pronoun_count = zip(*pronouns)\n",
    "    # getting repetition score\n",
    "    repetition_score = [repetition(x) for x in text]\n",
    "    # getting sentiment scores\n",
    "    sentiments = text.apply(get_sentiment)\n",
    "    polarity, subjectivity = zip(*sentiments)\n",
    "    # getting readability scores\n",
    "    readability = text.apply(get_readability)\n",
    "    reading_ease, kincaid_grade = zip(*readability)\n",
    "    # getting punctuation errors\n",
    "    punctuation_errors = [punctuation_error(x) for x in text]\n",
    "    \n",
    "    # Creating a dataframe from our features to merge it with our original dataframe\n",
    "    text_df = pd.DataFrame({'word_count': word_count, \n",
    "                            'uniq_word_count': uniq_word_count,\n",
    "                            'uniq_word_count_relative': uniq_word_count_rel,\n",
    "                            'stopwords_count': stopwords_count,\n",
    "                            'discourse_count': discourse_count, \n",
    "                            'pronouns_count': total_pronoun_count, \n",
    "                            'relative_pronouns_count': relative_pronoun_count,\n",
    "                            'repetition_score': repetition_score,\n",
    "                            'polarity': polarity,\n",
    "                            'subjectivity': subjectivity,\n",
    "                            'reading_ease': reading_ease,\n",
    "                            'kincaid_grade': kincaid_grade,\n",
    "                            'punctuation_count': punctuation_count,\n",
    "                            'punctuation_errors': punctuation_errors})\n",
    "    \n",
    "    # merging the dataframe with the orignal one\n",
    "    returned_df = pd.concat([orig_df, text_df], axis=1)\n",
    "    \n",
    "    return returned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005afa1",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83434cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "human_gpt = pd.read_csv('human_gpt_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_gpt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_gpt = human_gpt.drop('word_count', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190b3e8",
   "metadata": {},
   "source": [
    "## Applying functions to get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e31df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_gpt = get_para_and_sents(human_gpt['text'], human_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_gpt = get_paragraph_features(human_gpt['paragraphs'], human_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f523974",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_gpt = get_sentence_features(human_gpt['sentences'], human_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d91b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_gpt = get_text_features(human_gpt['text'], human_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb92ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "human_gpt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_gpt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc588c5",
   "metadata": {},
   "source": [
    "## Preparing prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring list of features for separate models to be trained on\n",
    "text_features = ['paragraph_count',\n",
    "       'words_per_paragraph_mean', 'words_per_paragraph_std',\n",
    "       'sentences_per_paragraph_mean', 'sentences_per_paragraph_std',\n",
    "       'sentence_count', 'words_per_sent_mean', 'words_per_sent_std', 'word_count']\n",
    "stylistic_features = ['uniq_words_per_sent_mean', 'uniq_words_per_sent_std',\n",
    "       'pos_tags_per_sentence_mean','uniq_word_count',\n",
    "       'uniq_word_count_relative', 'stopwords_count',\n",
    "       'discourse_count', 'pronouns_count', 'relative_pronouns_count',\n",
    "       'polarity', 'subjectivity', 'repetition_score', 'reading_ease',\n",
    "       'kincaid_grade']\n",
    "punctuation_features = ['punctuation_count','punctuation_errors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a72a9",
   "metadata": {},
   "source": [
    "## General model using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a880d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = human_gpt.drop(['label','text','paragraphs','sentences'], axis=1)\n",
    "y = human_gpt['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d13f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47842c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am declaring a parameter grid - what values I want to check while running the model.\n",
    "param_grid = {'n_estimators':[90, 100, 110, 120], 'max_features':[2, 3, 4],'bootstrap':[True, False],\n",
    "             'oob_score':[True, False]}\n",
    "\n",
    "# Here I am creating an instance of our classifier\n",
    "rfc = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "# Here I am passing my classifier to a model that is going to check it with those various parameters \n",
    "# that I specified above\n",
    "grid_model = GridSearchCV(rfc, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a60ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec2ee6",
   "metadata": {},
   "source": [
    "### Feature importances for the general model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f33225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = X.columns\n",
    "importances = grid_model.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef3d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'features': features, 'importances': importances})\\\n",
    ".sort_values(by='importances', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f20e0",
   "metadata": {},
   "source": [
    "### Final test scores for the general model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c51a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(grid_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a48c0",
   "metadata": {},
   "source": [
    "## Model based on text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = human_gpt[text_features]\n",
    "y = human_gpt['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fc97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e90806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am declaring a parameter grid - what values I want to check while running the model.\n",
    "param_grid = {'n_estimators':[90, 100, 110, 120], 'max_features':[2,3,4],'bootstrap':[True, False],\n",
    "             'oob_score':[True, False]}\n",
    "\n",
    "# Here I am creating an instance of our classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Here I am passing my classifier to a model that is going to check it with those various parameters \n",
    "# that I specified above\n",
    "grid_model = GridSearchCV(rfc, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5337dcf",
   "metadata": {},
   "source": [
    "### Feature importances of the text features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436264b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.columns\n",
    "importances = grid_model.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30906d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'features': features, 'importances': importances})\\\n",
    ".sort_values(by='importances', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5212c",
   "metadata": {},
   "source": [
    "### Final test scores for the text features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd792aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for some predictions\n",
    "y_pred = grid_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(grid_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff986f3",
   "metadata": {},
   "source": [
    "## Model based on stylistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = human_gpt[stylistic_features]\n",
    "y = human_gpt['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eec488",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am declaring a parameter grid - what values I want to check while running the model.\n",
    "param_grid = {'n_estimators':[90, 100, 110, 120], 'max_features':[2,3,4],'bootstrap':[True, False],\n",
    "             'oob_score':[True, False]}\n",
    "\n",
    "# Here I am creating an instance of our classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Here I am passing my classifier to a model that is going to check it with those various parameters \n",
    "# that I specified above\n",
    "grid_model = GridSearchCV(rfc, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bbdb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02acaa8c",
   "metadata": {},
   "source": [
    "### Feature importances of the stylistic features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.columns\n",
    "importances = grid_model.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'features': features, 'importances': importances})\\\n",
    ".sort_values(by='importances', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bcdcc5",
   "metadata": {},
   "source": [
    "### Final test scores for the stylistic features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for some predictions\n",
    "y_pred = grid_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a59126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(grid_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ff562",
   "metadata": {},
   "source": [
    "## Model based on punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c07b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = human_gpt[punctuation_features]\n",
    "y = human_gpt['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am declaring a parameter grid - what values I want to check while running the model.\n",
    "param_grid = {'n_estimators':[90, 100, 110, 120], 'max_features':[2,3,4],'bootstrap':[True, False],\n",
    "             'oob_score':[True, False]}\n",
    "\n",
    "# Here I am creating an instance of our classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Here I am passing my classifier to a model that is going to check it with those various parameters \n",
    "# that I specified above\n",
    "grid_model = GridSearchCV(rfc, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ff28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e8076",
   "metadata": {},
   "source": [
    "### Feature importances of the punctuation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943be754",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.columns\n",
    "importances = grid_model.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbb78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'features': features, 'importances': importances})\\\n",
    ".sort_values(by='importances', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc9b5d",
   "metadata": {},
   "source": [
    "### Final test scores for the punctuation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for some predictions\n",
    "y_pred = grid_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(grid_model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
